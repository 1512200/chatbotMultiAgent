## **ğŸ“˜ Documentation â€“ Chatbot Multi-Agent**
Voila l'interface :

![Supabase Studio](images/interface.png)

Lorsque lâ€™utilisateur pose une question:

1. ğŸ“© La question reÃ§ue  
2. ğŸ§  Lâ€™intention dÃ©tectÃ©e (exemple : `onboarding`)  
3. ğŸ¤– Lâ€™agent correspondant sÃ©lectionnÃ© (ici **Onboarding Agent**)  
4. ğŸ“¤ La rÃ©ponse gÃ©nÃ©rÃ©e par cet agent, renvoyÃ©e Ã  lâ€™utilisateur  

Voici un exemple dâ€™exÃ©cution dans le terminal :


![Supabase Studio](images/serv.png)


et voila la rÃ©ponse: 

![Supabase Studio](images/rÃ©ponse.png)





1. _Cloner le projet_

`git clone git@gitlab.com:elhadjiatikapfa/chatbot_multi-agent.git`
`cd chatbot_multi-agent`

2. _CrÃ©er un environnement virtuel_

`python -m venv venv`

3. _Activer l'environnement_ 

`.\venv\Scripts\activate`

4. _Installer les dÃ©pendances_ 

`pip install -r requirements.txt`

#### Installation et configuration de Supabase (local)

1. _Installer la CLI supabase_

TÃ©lÃ©charge et install CLI oficielle 
[Guide installation](https://supabase.com/docs/guides/local-development/cli/getting-started)


2. CrÃ©e un projet et initialiser supabase

`mkdir Chatbot`
`cd Chatbot`
`supabase start`

3. AccÃ©der Ã  Supabase Studio

`http://localhost:54323`

Ce lien ouvre Supabase Studio (interface web) oÃ¹ tu peux :

-- CrÃ©er des tables

-- Explorer tes donnÃ©es

-- GÃ©rer tes policies (RLS)

![Supabase Studio](images/supabase studio.png)

4. Variables d'environnement

`supabase status`

SUPABASE_URL=http://localhost:54321

SUPABASE_ANON_KEY=...

SUPABASE_SERVICE_ROLE_KEY=...

â¡ï¸ Ces infos doivent Ãªtre ajoutÃ©es dans le projet(agent/) pour se connecter Ã  la base.

dans supabase studio il doit crÃ©er les tables (sales , tutoring, reclamation)

#### Installation et utilisation d'un llm local

ğŸ‘‰ ollama est un outil qui permet de lancer llaMa local 
- TÃ©lÃ©charge depuis https://ollama.ai
- installe-le

TÃ©lÃ©charger un modÃ©le (llaMA 3, mistral ...)

`ollama pull llama3` ou `ollama pull mistral`

#### Lancer le serveur

`uvicorn app:main --reload`

Le serveur dÃ©marre sur : http://127.0.0.1:8000



















