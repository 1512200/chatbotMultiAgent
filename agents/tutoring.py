from langchain_huggingface import HuggingFaceEmbeddings
from supabase import create_client
import requests

# üîπ Config Supabase
SUPABASE_URL = "http://127.0.0.1:54321"
SUPABASE_KEY = ""  # ‚ö† utiliser cl√© Service Role c√¥t√© backend uniquement
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# üîπ Charger le mod√®le 512 dimensions
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/clip-ViT-B-32-multilingual-v1"
)

# üîπ Fonction g√©n√©ration avec Mistral local
def generate_with_mistral(prompt):
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "mistral",
            "prompt": prompt,
            "stream": False
        }
    )
    return response.json()["response"]

# üîπ Fonction de r√©ponse via Supabase
def answer_tutoring(question, k=3):
    # 1Ô∏è‚É£ Encoder la question en vecteur 512D
    q_vec = embeddings.embed_query(question)

    # 2Ô∏è‚É£ Recherche dans Supabase
    # Si tu as une colonne 'embedding' de type `vector(512)`
    results = supabase.rpc(
        "match_documents_tutoring",  # ta fonction SQL de recherche vectorielle
        {
            "query_embedding": q_vec,
            "match_count": k
        }
    ).execute()

    matches = results.data
    if not matches:
        return "Je n‚Äôai pas trouv√© de r√©ponse pertinente."

    # 3Ô∏è‚É£ Construire le contexte
    context = "\n".join([m["content"] for m in matches if m.get("content")])

    # 4Ô∏è‚É£ Prompt vers Mistral
    prompt = f"""Tu es un assistant de formation.
Utilise les √©l√©ments suivants pour r√©pondre √† la question :
{context}

Question : {question}

R√©ponds de fa√ßon claire, professionnelle et utile."""
    return generate_with_mistral(prompt)


